# Introduction

When I joined my current employer early 2014, I was handed over the ownership of a dashboarding and reporting solution. In the state the software was in, dashboards and reports alike would query the live transactional database directly. This approach was extremely simple and worked pretty well when there were only a handful of people using the system. Things started to go awry when the website started serving more and more actual paying customers. The dasboards and reports would take a long time to load, or would even time out. To make things even worse, because of business owners frantically refreshing their dashboards, customers would start experiencing failing transactions.

With production on fire, your first instinct is to extinguish the fire as quickly as possible. We hastily disabled features that were not mission-critical, rewrote problematic queries, fine-tuned indexes and added some caching on top. This was a small investement, but had a big immediate impact on the performance of the dashboards. This hardly brought any relief for the reports though. They queried a much larger data set, and noticeably got slower day by day. Here, we introduced an intermediary layer that ran nightly roll-ups so that we could query an already aggregated, highly compressed data set.

These changes had a big pay-off and were stable for a good while. However, we still had a set of problems. When you're seeing business 24/7, the scheduled nightly roll-ups still impacted a small set of customers.

Having to tune schemas and indexes in a way so that they can be used to both serve transactional and dashboarding workloads makes things more complex and fragile than they should be. Dashboarding queries had to be implemented and monitored carefully, since having a single dashboarding query go rogue could have a negative effect on transactional workloads.

With some big and non-trivial features in the pipeline, we set out some time to port the existing functionality onto infrastructure that could handle our ever demanding needs better. We settled for Amazon Redshift quite quickly.

Having experience running parts of our infrastructure on AWS, AWS is more often than not the first place we look. Low operational requirements, low-cost and room to scale up to petabytes of data, reduce the buy-in needed to build a proof of concept significantly.

In comparison to other Big Data products, Redshift supports Standard SQL. SQL is not always the most elegant language, but its declarative nature does a good job hiding most of the complexities of running massive parallel queries on a cluster of machines. It is ubiquitous in the developer world and well understood by most.

All these parameters combined allowed us to to gain enough confidence to validate our decision to go with Redshift in just a couple of weeks.

We got decent results very early on, but failing to fully understand all Redshift core concepts left room for improvement. Putting the work in to have a solid grasp on the fundamentals early on would definitely be a high-return investment. Shortly after committing to this goal, I started working on this piece of investigative writing - writing down what we learned, tuning our own setup along the way. By the time I finished writing the last chapter, we ported most of the functionality. This took us less than two months. Except for a handful of important schema design decisions, Redshift offered surprisingly little resistance. We were even able to drop the whole intermediary roll-up layer, saving a few thousands lines of code and removing another operational concern. Complexity dropped back to the level of the early days, where we could just directly query the raw data set in its completeness, without having to worry about things being too slow or affecting customer transactions.

My text editor counts almost 10000 words in this document, making it something the average reader comfortably works through in less than one hour. I hope it can save you the time of diving into a 400 page thick book, yet telling a more cohesive and compact story than reading the technical documentation.

The path I followed, takes you from the storage layer all the way up to query processing and workload management - layering one concept on top of the other. By the time you finish reading, you should have a solid understanding of the high-level architecture of a Redshift cluster and have a good idea of how to make it work in the real world. If you're a technical decision maker, this text should allow you to make a well-informed decision on whether Amazon Redshift might be a sane addition to your stack.

I assume most people interested in this content have some experience working with a relational transactional database. Although the text contains some SQL queries you can play with on your own cluster, setting up a cluster, making provisioning decisions, monitoring overall health or connecting to the cluster are not covered. The AWS console does quite a good job of guiding you through this process. I'd advice you to go through the process of setting up a cluster, to immediately destroy it before you start reading. It should only take minutes.

I hope that this essay can teach you as much about Amazon Redshift as it has taught me.
